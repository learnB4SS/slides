<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Bayesian Data  Analysis  for  Speech Sciences</title>
    <meta charset="utf-8" />
    <meta name="author" content="Timo Roettger, Stefano Coretta and Joseph Casillas" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/hygge.css" rel="stylesheet" />
    <link rel="stylesheet" href="https://learnb4ss.github.io/b4ss-theme/css/b4ss.css" type="text/css" />
    <link rel="stylesheet" href="https://learnb4ss.github.io/b4ss-theme/css/b4ss-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Bayesian Data <br>Analysis<br> for <br>Speech Sciences
## Sampling from the posterior
### Timo Roettger, Stefano Coretta and Joseph Casillas
### LabPhon workshop
### 2021/07/06 (updated: 2021-06-30)

---










count: false
background-image: url(./libs/mira1.png)
background-size: contain
background-position: 50% 100%

???

In this session we are going to talk about model assessment

That is, what to do after you have run your model and you want to know about goodness of fit

We are going to talk about Markov chains and some diagnostics like Rhat and effective sample size, as well as best practices for correcting for divergent transitions

The purpose of this session is to help you build intuition about how the sampling process works and to know when your model is poor... luckily BRMS makes that quite obvious

If you have young children or are fans of Disney junior, then you might recognize Mira, royal detective. 

If not, Mira is the star of my daughters favorite mystery cartoon. 
She is the brave and resourceful royal detective in a fictional kingdom called Jalpur 

She travels around the kingdom solving mysteries and I think this is a bit similar to what it's like, especially at first, when building and assessing models

So, the plan is that we are going to be like Mira and solve some Bayesian mysteries and we'll get a good introduction to the topic, though it won't be exhaustive, and at the end I'll include some references for further reading

---
class: center, middle
background-image: url(./libs/chain1.png)
background-size: contain

???

We hinted before that Bayesian regression involves complicated sampling algorithms that help us approximate a posterior distribution

You may have heard about one class of these algorithms before, called Markov chain Monte Carlo methods

---
count: false
class: center, middle
background-image: url(./libs/chain2.png)
background-size: contain

# Markov chain Monte Carlo (MCMC)

--

### When we fit a model using `brm()` we run multiple markov chains

--

### The chains sample the parameter space (more on this in a bit)

--

### We assess the chains to get an idea of how well the model converged

--

### If the sampling procedure is sub-optimal we can inspect the chains and check several diagnostics

---
class: center, middle

&lt;img src="index_files/figure-html/good-chains-1.png" width="936" /&gt;

???

They we usually do this is by looking at trace plots

Here we see the values of a parameter on the vertical axis, in this case the estimate of the intercept, and the sample iteration on the horizontal axis

It's hard to tell, but there are 4 different lines that are moving from left to right

This is an example of what we could call good, healthy chains, the algorithm worked... we say the chain have mixed well

As we look from left to right we see what many describe as a hairy caterpillar 

So when we inspect our chains, this is what we want

---
class: right, middle
background-image: url(https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ75CzFCTvGIxClCFzrJ6iF2YA6yzxVN-GraQ&amp;usqp=CAU)
background-size: 800px

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

### .grey[A promising young Markov chain]

???

This is also a healthy chain

---
class: center, middle

&lt;img src="index_files/figure-html/bad-chains-1.png" width="936" /&gt;

???

If the sampling process does not work properly we might see something like this

If you follow any given line you will notice that they do not sample the parameter space like in the previous plot

That is, there is not a lot of random up and down movement... there is no hairy caterpillar... this means the chains have not mixed well

Now, obviously determining if your trace plot looks like a hairy caterpillar is not precisely an exact science, but luckily there are also diagnostics that tell us about the chains performance

---

# Model assessment

### Rhat and effective sample size

???

Two of those diagnostics that we're going to talk about Rhat and effective sample size

You probably noticed over the course of the last two days that when we examine a summary of a brms model we see something like this...

--


```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: articulation_rate ~ attitude 
##    Data: polite (Number of observations: 224) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept       6.88      0.11     6.67     7.10 1.00     3677     2815
## attitudepol    -0.41      0.15    -0.70    -0.11 1.00     3657     2873
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.15      0.05     1.05     1.26 1.00     3684     3013
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

???

This is the same model we fit earlier today looking at articulation rate as a function of attitude

Explain "Samples" line

Now I'd like to call your attention to this area of the table...

--

background-image: url(./libs/mira2.png), url(https://raw.githubusercontent.com/jvcasillas/media/master/general/img/circle.png)
background-size: 800px, 410px
background-position: 150% 50%, 72% 72%

???

You have already noticed these three columns on the right-hand side: Rhat, bulk ESS and tail ESS

What is all this about?

Rhat is a convergence diagnostic that compares estimates between and within chains (Im simplifying)

The key thing to remember is that this value should less than 1.01, if not, it is a sign the chains did not mix well, i.e., between- and within-chain estimates don't agree

In this case, we can see that all the rhat values are 1.00

With regard to Effective Sample size, this metric "captures the number of independent draws that have the same amount of information as the dependent sample obtained by the MCMC algorithm... 

That's a lot to unpack if you are new at this... the important thing to remember is that these numbers should be high (the max is 4k, or the number of iterations)

It's helpful to compare with a poorly fitted model

---

# Model assessment

### Rhat and effective sample size (bad example)

???

This model fits the same parameters but I purposely gave it vary bad priors

--

background-image: url(./libs/mira2.png), url(https://raw.githubusercontent.com/jvcasillas/media/master/general/img/circle.png)
background-size: 800px, 410px
background-position: 150% 50%, 72% 72%


```
##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: articulation_rate ~ attitude 
##    Data: polite (Number of observations: 224) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Population-Level Effects: 
##             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept       0.01      0.00     0.01     0.02 1.14      181       77
## attitudepol    -0.00      0.00    -0.00     0.00 1.10       99       46
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.32      0.00     0.32     0.32 1.03      137      156
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).
```

???

Notice that the Rhat values are higher than 1.01, and the ESS are very low, less than 100x 4 chains

---
class: inverse
background-image: url(./libs/warning.png)
background-size: contain

???

Luckily, when something like this happens BRMS will also give us warnings

Note that it specifically suggests caution when analyzing the results and gives helpful suggestions

1. Run more iterations on each chain
2. Take another look at your priors (the problem here)

Notice that there is another warning (part 2): Divergent transitions

In this case BRMS also gives us a suggestion for combating this problem (increase delta), as well as a link with more information

So we already know what the solutions is, but let's unpack the problem...


---
class: middle, center
background-image: url(https://d23.com/app/uploads/2018/11/1180w-600h_112918_mira-royal-detective-announce-780x440.jpg)
background-size: 800px
background-position: 50% 40%

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;
&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

# Divergent transitions

---

SAMPLES IMAGE

???

Again this is an issue related to sampling the parameter space, so let's talk about parameter space...

---
class: center, middle

# So what is parameter space?

???

what is it?

You  may have noticed that I say this quite a bit

---
class: middle, center

&lt;img src="index_files/figure-html/data-space-1.png" width="864" /&gt;

???

Explain data space

---

&lt;br&gt;&lt;br&gt;

&lt;iframe src="https://jvcasillas.shinyapps.io/shiny_parameter_space/" style="border:none;" width="100%" height="100%"&gt;


???

compare data space with parameter space

This is an extremely simple example in which we only fit a couple of parameters and we can easily visualize the parameter space in two dimensions

---
background-image: url(./libs/dark_density_posterior1.png)
background-size: contain
background-color: black

???

or 3 dimensions

---
background-image: url(./libs/dark_density_posterior2.png)
background-size: contain
background-color: black

???

EXPLAIN HOW SAMPLING WORKS HERE

---

&lt;iframe src="https://chi-feng.github.io/mcmc-demo/app.html" style="border:none;" width="100%" height="100%"&gt;

???

This helps us develop intuition about the problem but remember we rarely fit such simple models

It isn't unusual to fit models with 100's or even 1000's of parameters

As humans, we struggle even imagining what a high dimensional parameter space might look like... so plotting can't help us there

---

# Model assessment

### Dealing with divergencies

HILL IMAGE

&lt;!--


Calculating a posterior is computationally costly

This is part of the reason why we didn't start seeing complex Bayesian models until relatively recently

As computational power has increased so has our ability to approximate posterior distributions

Furthermore, we've developed faster methods of sampling from the parameter space

The "meat and potatos" of this workshop has already been served, if you will. 
At this point you already know quite regarding what BDA is all about

In this penultimate session we are going to build on our intuitions regarding what it means to sample from parameter space and how this is done, but know that you have already finished the principle components of this workshop. 
This final session is merely to give you more of an idea of how some of the "under the hood" machinery works as you move forward as Bayesians








If there’s a random way to do something, there’s usually a less random way that is both better and requires more thought. Instead of making random proposals, suppose instead that you run a physics simulation. This is going to sound crazy, but it isn’t. Your vector of parameters is now a particle in n-dimensional space. The surface in this space is a giant n-dimensional bowl. The shape of the bowl is determined by the shape of the logarithm of the target distribution. If the target is a nice Gaussian, for example, then the log-Gaussian is a smooth parabolic bowl like this (in one-dimension):


To make things a little crazier, suppose that this surface is frictionless. Now what we do is flick the particle in a random direction. It will frictionlessly flow across the bowl, eventually turning around. If we take samples of the particle’s position along its path, before flicking it off in another random trajectory, then we can learn about the shape of the surface.

This is the principle behind Hamiltonian Monte Carlo. It will be easier to see it in action. Here is another simulation, this time using Hamiltonian Monte Carlo, again on the two-dimensional Gaussian target. The paths are flicks of the particle, and the green arrows again represent accepted proposals.

Now the proposals are both within the high-probability region of the target—so many fewer proposals are rejected—and the proposals can get far away from their starting point, so that the chain efficiently explores the whole shape of the target in less time. Effectively, it flows across the target and maps out its whole shape much faster.

The cost of all this elegance is needing more information about the target. Hamiltonian Monte Carlo does a lot more calculation. But it also needs fewer samples to get a good image of the target. 
--&gt;

---

# Model assessment

### More information

- Hoffman and Gelman (2011) “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” arxiv.org/abs/1111.4246

- Betancourt. “Conceptual Introduction to Hamiltonian Monte Carlo” arxiv.org/abs/1701.02434

- http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/

- More on Rhat and Bulk ESS and Tail ESS: Vehtari et al. (2020)

Stan warnings: https://mc-stan.org/misc/warnings.html

---
count: false
background-image: url(./libs/mira.gif)
background-size: contain
background-color: black


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://learnb4ss.github.io/b4ss-theme/js/b4ss_xaringan.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
