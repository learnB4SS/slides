---
title: "Bayesian Data <br>Analysis<br> for <br>Speech Sciences"
subtitle: "Sampling from the posterior"
author: "Timo Roettger, Stefano Coretta and Joseph Casillas"
institute: "LabPhon workshop"
date: "2021/07/12 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["hygge", "https://learnb4ss.github.io/b4ss-theme/css/b4ss.css", "https://learnb4ss.github.io/b4ss-theme/css/b4ss-fonts.css"]
    lib_dir: libs
    nature:
      beforeInit: ["https://learnb4ss.github.io/b4ss-theme/js/b4ss_xaringan.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: 16:9
---

```{r setup, include=FALSE}
options(htmltools.dir.version=F)
knitr::opts_chunk$set(fig.retina=2, cache=F, warning=F, message=F)
```

```{r xaringan-extra-all-the-things, echo=F, eval=F}
xaringanExtra::use_xaringan_extra(
  c("tile_view", "panelset", "editable"
    #"scribble", "search", "webcam"
    )
)
```

```{r, 'helpers', echo=F}
source(here::here("assets", "helpers.R"))
```

```{r, load_refs, echo=FALSE, cache=FALSE, warning=F, message=F}
bib <- ReadBib(here("assets", "b4ss_refs.bib"), check = FALSE)
ui <- "- "
```


- brms output
	- ess
	- rhat
- chains assessment
- divergences
- sampling




???

Calculating a posterior is computationally costly

This is part of the reason why we didn't start seeing complex Bayesian models until relatively recently

As computational power has increased so has our ability to approximate posterior distributions

Furthermore, we've developed faster methods of sampling from the parameter space

The "meat and potatos" of this workshop has already been served, if you will. 
At this point you already know quite regarding what BDA is all about

In this penultimate session we are going to build on our intuitions regarding what it means to sample from parameter space and how this is done, but know that you have already finished the principle components of this workshop. 
This final session is merely to give you more of an idea of how some of the "under the hood" machinery works as you move forward as Bayesians

---
class: center, middle

# So what is parameter space?

???

So, what is parameter space?

You  may have noticed that I say this quite a bit

---
class: middle, center

```{r, data-space, echo=F, fig.width=12}
# Set seed to match shiny app data
set.seed(20210302)
dat <- tibble(
  x = rnorm(25, 0, 1), 
  y = 0 + x * 0.5 + rnorm(25, 0, 1)
  )

# Fit model
mod <- lm(y ~ x, data = dat)

# Data space plot
broom::augment(mod) %>% 
  ggplot(., aes(x = x, y = y)) + 
    geom_vline(xintercept = 0, lty = 3) + 
    geom_hline(yintercept = 0, lty = 3) + 
    geom_point(aes(fill = .resid), pch = 21, size = 6, show.legend = F) + 
    scale_fill_gradient2() + 
    geom_abline(intercept = coef(mod)[1], slope = coef(mod)[2], 
                color = "#cc0033", size = 1.2) + 
    coord_cartesian(xlim = c(-2.5, 2.5), ylim = c(-2.5, 2.5)) + 
    labs(title = "Data space") + 
    b4ss_bw(base_size = 24)

```

---

<br><br>

<iframe src="https://jvcasillas.shinyapps.io/shiny_parameter_space/" style="border:none;" width="100%" height="100%">

---
background-image: url(./libs/dark_density_posterior1.png)
background-size: contain
background-color: black

---
background-image: url(./libs/dark_density_posterior2.png)
background-size: contain
background-color: black

---

<iframe src="https://chi-feng.github.io/mcmc-demo/app.html" style="border:none;" width="100%" height="100%">



<!--
If there’s a random way to do something, there’s usually a less random way that is both better and requires more thought. Instead of making random proposals, suppose instead that you run a physics simulation. This is going to sound crazy, but it isn’t. Your vector of parameters is now a particle in n-dimensional space. The surface in this space is a giant n-dimensional bowl. The shape of the bowl is determined by the shape of the logarithm of the target distribution. If the target is a nice Gaussian, for example, then the log-Gaussian is a smooth parabolic bowl like this (in one-dimension):


To make things a little crazier, suppose that this surface is frictionless. Now what we do is flick the particle in a random direction. It will frictionlessly flow across the bowl, eventually turning around. If we take samples of the particle’s position along its path, before flicking it off in another random trajectory, then we can learn about the shape of the surface.

This is the principle behind Hamiltonian Monte Carlo. It will be easier to see it in action. Here is another simulation, this time using Hamiltonian Monte Carlo, again on the two-dimensional Gaussian target. The paths are flicks of the particle, and the green arrows again represent accepted proposals.

Now the proposals are both within the high-probability region of the target—so many fewer proposals are rejected—and the proposals can get far away from their starting point, so that the chain efficiently explores the whole shape of the target in less time. Effectively, it flows across the target and maps out its whole shape much faster.

The cost of all this elegance is needing more information about the target. Hamiltonian Monte Carlo does a lot more calculation. But it also needs fewer samples to get a good image of the target. 
-->



Hoffman and Gelman (2011) “The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.” arxiv.org/abs/1111.4246

Betancourt. “Conceptual Introduction to Hamiltonian Monte Carlo” arxiv.org/abs/1701.02434

http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/
